# AI-Powered Document Search Engine

Проект для создания поисковой системы по PDF-документам с использованием AI. Позволяет загружать PDF-файлы, индексировать их содержимое и искать информацию с помощью естественного языка.

## Архитектура

Проект построен на модульной архитектуре RAG (Retrieval-Augmented Generation):

1. **Ingest** (`ingest.py`) - Извлечение текста из PDF с помощью PyMuPDF
2. **Chunk** (`chunk.py`) - Разбиение текста на фрагменты с перекрытием
3. **Embed** (`embed.py`) - Преобразование текста в векторные представления
4. **Index** (`index.py`) - Создание индекса для быстрого поиска (FAISS)
5. **Search** (`search.py`) - Поиск релевантных фрагментов по запросу
6. **Generate** (`generate.py`) - Генерация ответов с помощью OpenAI
7. **Frontend** (`app.py`) - Веб-интерфейс на Streamlit

## Установка

1. Клонируйте репозиторий:
```bash
git clone <repository-url>
cd AI-Powered Document Search
```

2. Создайте виртуальное окружение:
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# или
venv\Scripts\activate  # Windows
```

3. Установите зависимости:
```bash
pip install -r requirements.txt
```

4. Установите переменную окружения для OpenAI API:
```bash
export OPENAI_API_KEY="your-api-key-here"
```

## Использование

### 1. Подготовка данных

Создайте папку `pdfs` и поместите туда PDF-файлы для индексации:
```bash
mkdir pdfs
# Скопируйте PDF-файлы в папку pdfs/
```

### 2. Индексация документов

Запустите пайплайн индексации поэтапно:

```bash
# 1. Извлечение текста из PDF
python ingest.py

# 2. Разбиение на чанки
python chunk.py

# 3. Создание эмбеддингов
python embed.py

# 4. Построение индекса
python index.py
```

### 3. Запуск веб-приложения

```bash
streamlit run app.py
```

Приложение будет доступно по адресу: http://localhost:8501

### 4. Использование через командную строку

```bash
# Поиск без веб-интерфейса
python search.py

# Генерация ответа
python generate.py
```

## Структура файлов

```
AI-Powered Document Search/
├── requirements.txt          # Зависимости Python
├── ingest.py                # Извлечение текста из PDF
├── chunk.py                 # Разбиение на фрагменты
├── embed.py                 # Создание эмбеддингов
├── index.py                 # Построение индекса FAISS
├── search.py                # Поиск по индексу
├── generate.py              # Генерация ответов
├── app.py                   # Streamlit приложение
├── agents.md                # Документация
├── pdfs/                    # Папка с PDF-файлами
├── docs.index               # FAISS индекс (создается автоматически)
├── chunks.pkl               # Метаданные чанков (создается автоматически)
└── vectors.npy              # Векторы эмбеддингов (создается автоматически)
```

## Настройка

### Параметры чанков
В `chunk.py` можно изменить размер чанков и перекрытие:
```python
chunk_document(doc, size=1000, overlap=200)  # size=размер, overlap=перекрытие
```

### Модель эмбеддингов
В `embed.py` и `search.py` можно заменить модель:
```python
model = SentenceTransformer('all-MiniLM-L6-v2')  # Замените на другую модель
```

### Количество результатов поиска
В `search.py` и `generate.py`:
```python
search_docs(query, top_k=3)  # Измените top_k для большего количества результатов
```

## Возможные улучшения

1. **Облачное хранение**: Замена FAISS на Pinecone, Qdrant или Milvus
2. **Асинхронная обработка**: Добавление multiprocessing для больших коллекций
3. **Умное разбиение**: Разбиение по структуре документа (заголовки, абзацы)
4. **Поддержка форматов**: Добавление docx, html, txt
5. **Чат-история**: Сохранение контекста диалога
6. **Персонализация**: Адаптация под пользователя
7. **Docker**: Контейнеризация для легкого деплоя
8. **Тесты**: Unit и интеграционные тесты
9. **Мониторинг**: Логирование и метрики производительности
10. **Безопасность**: Аутентификация и авторизация

## Требования

- Python 3.8+
- OpenAI API ключ
- Минимум 4GB RAM для обработки больших коллекций
- Свободное место на диске для индексов (зависит от размера коллекции)

## Лицензия

MIT License