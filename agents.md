# AI-Powered Document Search Engine

Проект для создания поисковой системы по PDF-документам с использованием AI. Позволяет загружать PDF-файлы, индексировать их содержимое и искать информацию с помощью естественного языка.

## Архитектура

Проект построен на модульной архитектуре RAG (Retrieval-Augmented Generation):

1. **Ingest** (`ingest.py`) - Извлечение текста из PDF с помощью PyMuPDF. **Включает этап автоматической очистки от повторяющихся шапок. Логика настраивается в файле `header_templates.json`.**
2. **Chunk** (`chunk.py`) - Разбиение текста на фрагменты с перекрытием
3. **Embed** (`embed.py`) - Преобразование текста в векторные представления
4. **Index** (`index.py`) - Создание индекса для быстрого поиска (FAISS)
5. **Search** (`search.py`) - Поиск релевантных фрагментов по запросу
6. **Generate** (`generate.py`) - Генерация ответов с помощью OpenAI
7. **Frontend** (`app.py`) - Веб-интерфейс на Streamlit

## Установка

1. Клонируйте репозиторий:
```bash
git clone <repository-url>
cd AI-Powered Document Search
```

2. Создайте виртуальное окружение:
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# или
venv\Scripts\activate  # Windows
```

3. Установите зависимости:
```bash
pip install -r requirements.txt
```

4. Установите переменную окружения для OpenAI API:
```bash
export OPENAI_API_KEY="your-api-key-here"
```

## Использование

### 1. Подготовка данных

Создайте папку `pdfs` и поместите туда PDF-файлы для индексации:
```bash
mkdir pdfs
# Скопируйте PDF-файлы в папку pdfs/
```

### 2. Индексация документов

Запустите пайплайн индексации поэтапно:

```bash
# 1. Извлечение текста из PDF
python ingest.py

# 2. Разбиение на чанки
python chunk.py

# 3. Создание эмбеддингов
python embed.py

# 4. Построение индекса
python index.py
```

### 3. Запуск веб-приложения

```bash
streamlit run app.py
```

Приложение будет доступно по адресу: http://localhost:8501

### 4. Использование через командную строку

```bash
# Поиск без веб-интерфейса
python search.py

# Генерация ответа
python generate.py
```

### Структура папок (src-layout)
*Использование `src-layout` — это современный стандарт, который решает множество проблем с импортами и гарантирует, что проект запускается так же, как он будет установлен в виде пакета. Текущий проект был реорганизован в соответствии с этой структурой.*

AI-Powered Document Search/
├── app.py                # UI-приложение Streamlit (точка входа)
├── scripts/
│   └── run_indexing.py   # Скрипт для запуска полного цикла индексации
├── src/
│   └── document_search/    # Основной пакет приложения
│       ├── __init__.py     # Инициализация пакета
│       ├── chunk.py        # Логика разбиения на чанки
│       ├── embed.py        # Логика создания эмбеддингов
│       └── ... (остальные .py файлы)
├── requirements.txt      # Зависимости проекта
└── ...

## Настройка

### Параметры чанков
В `chunk.py` можно изменить размер чанков и перекрытие:
```python
chunk_document(doc, size=1000, overlap=200)  # size=размер, overlap=перекрытие
```

### Модель эмбеддингов
В `embed.py` и `search.py` можно заменить модель:
```python
model = SentenceTransformer('all-MiniLM-L6-v2')  # Замените на другую модель
```

### Количество результатов поиска
В `search.py` и `generate.py`:
```python
search_docs(query, top_k=3)  # Измените top_k для большего количества результатов
```

### Очистка шапок PDF
В файле `header_templates.json` можно настроить удаление повторяющихся шапок из документов. В массив `header_labels` нужно добавлять текстовые "метки", которые присутствуют в шапке. Скрипт будет автоматически удалять строки с этими метками и следующие за ними строки со значениями на всех страницах, кроме первой.

## Выбор Модели Эмбеддингов (Embedding Model Selection)

Качество поиска в RAG-системе напрямую зависит от качества эмбеддингов. Выбор правильной модели — ключевой шаг.

### Что такое `SentenceTransformer('all-MiniLM-L6-v2')`?

Это быстрая и универсальная модель для создания эмбеддингов. Она является отличной отправной точкой, так как обеспечивает хороший баланс между скоростью, потреблением ресурсов и качеством. `MiniLM` в названии означает, что это "дистиллированная" (уменьшенная и быстрая) версия большой языковой модели.

### Категории моделей

Модели для эмбеддингов можно условно разделить на несколько категорий:

1.  **Легковесные и быстрые:** Как `all-MiniLM-L6-v2`. Идеальны для быстрого старта, прототипирования и систем с ограниченными ресурсами.
2.  **Сбалансированные:** Модели, предлагающие лучшее качество при умеренном потреблении ресурсов. Отличный выбор для большинства задач на современном оборудовании. Пример: `bge-base-en-v1.5`.
3.  **Максимальное качество:** Крупные модели, занимающие верхние строчки в бенчмарках (например, MTEB Leaderboard). Они требуют значительных ресурсов (часто GPU), но обеспечивают наилучшую точность. Пример: `bge-large-en-v1.5`, `NV-Embed-v2`.
4.  **Многоязычные:** Специализированные модели для работы с текстами на нескольких языках. Пример: `paraphrase-multilingual-mpnet-base-v2`.

### Лучшие практики выбора

- **Начинайте с простого:** Используйте `all-MiniLM-L6-v2` как baseline.
- **Ориентируйтесь на задачу:** Для поиска по документам (Retrieval) выбирайте модели с высокими показателями в этой категории на **MTEB Leaderboard**.
- **Балансируйте качество и ресурсы:** Учитывайте ваше оборудование, требования к скорости ответа и стоимость.
- **Тестируйте на своих данных:** Лучшая модель для вашего проекта — та, которая лучше всего работает именно на ваших документах.

### Рекомендация для MacBook Air M4 (32GB RAM)

Ваша система оснащена чипом M4, чей Neural Engine почти в 2.5 раза быстрее, чем у M2. Это позволяет с комфортом использовать самые качественные модели.

- **Основная рекомендация:** **`bge-large-en-v1.5`**. Благодаря мощности M4, вы получите эмбеддинги высочайшего качества при сохранении отличной скорости работы. Это лучший выбор для максимальной точности поиска.
- **Если скорость критична:** **`bge-base-en-v1.5`**. Остается прекрасным вариантом, если вы обрабатываете огромные объемы данных и каждая миллисекунда имеет значение. Однако, с чипом M4 компромисс в сторону скорости, скорее всего, не понадобится.

## Возможные улучшения

1. **Облачное хранение**: Замена локального FAISS на управляемые векторные базы данных, такие как Pinecone, Qdrant или Milvus, для лучшей масштабируемости.
2. **Гибридный поиск**: Интеграция поиска по ключевым словам (например, BM25) с векторным поиском для улучшения релевантности за счет сочетания семантической близости и точных совпадений.
3. **Переранжирование (Re-ranking)**: Добавление легковесной модели для пересортировки топ-K результатов из поиска перед их подачей в LLM, чтобы повысить точность контекста.
4. **Асинхронная обработка**: Использование `multiprocessing` или `asyncio` для ускорения индексации больших коллекций документов.
5. **Умное разбиение**: Переход от фиксированных чанков к разбиению на основе структуры документа (заголовки, абзацы, таблицы).
6. **Поддержка форматов**: Расширение поддержки на другие форматы, такие как `.docx`, `.html`, `.txt`.
7. **Управление конфигурацией**: Вынос параметров (имена моделей, размеры чанков, `top_k`) в единый конфигурационный файл (например, `config.yaml` или `.env`).
8. **Чат-история**: Реализация сохранения контекста диалога для более естественного взаимодействия.
9. **CI/CD и Тестирование**: Настройка CI/CD пайплайна (например, через GitHub Actions) для автоматического запуска тестов и деплоя. Написание Unit и интеграционных тестов.
10. **Оценка качества (Evaluation)**: Создание пайплайна для оценки качества RAG-системы с использованием метрик, таких как `context precision/recall` и `faithfulness`.
11. **Docker-контейнеризация**: Упаковка приложения в Docker-контейнер для упрощения развертывания и обеспечения консистентности окружения.
12. **Безопасность**: Добавление аутентификации и авторизации для контроля доступа к приложению.
13. **Мониторинг**: Интеграция с системами логирования и мониторинга для отслеживания производительности и ошибок.

## Требования

- Python 3.8+
- OpenAI API ключ
- Минимум 4GB RAM для обработки больших коллекций
- Свободное место на диске для индексов (зависит от размера коллекции)

## Сопровождение и Обновление

### Автоматическое обновление документации

Этот файл (`agents.md`) должен служить актуальным источником информации о проекте. При добавлении или существенном изменении функциональности (например, добавлении новых функций, изменении архитектуры или шагов использования), **этот документ должен автоматически обновляться** для отражения этих изменений.

**Ключевое правило:** Любое изменение логики, промпта или набора инструментов самого AI-агента должно в первую очередь отражаться в этом документе.

### Устранение неполадок

В процессе разработки были выявлены и решены некоторые проблемы. Описание этих проблем и шаги по их решению задокументированы в файле [TROUBLESHOOTING.md](./TROUBLESHOOTING.md).

## Лицензия

MIT License