"""
–ú–æ–¥—É–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç llm_client –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞ –∏ –º–æ–¥–µ–ª–∏.
"""
from .llm_client import get_llm_client_and_model

# –ü–æ–ª—É—á–∞–µ–º –∫–ª–∏–µ–Ω—Ç –∏ –∏–º—è –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥—É–ª—è
client, model_name = get_llm_client_and_model()

def ask(question, context):
    prompt = f"""
Context:
{context}
Question: {question}
Answer in a short paragraph.
"""
    response = client.chat.completions.create(
        model=model_name,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7,
    )
    return response.choices[0].message.content

if __name__ == "__main__":
    from .search import search_docs
    question = "Explain transfer learning"
    top_chunks = search_docs(question)
    context = "\n\n".join([c["text"] for c in top_chunks])
    answer = ask(question, context)
    print("üí° Answer:", answer)